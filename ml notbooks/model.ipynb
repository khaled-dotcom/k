{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy80zpsI3MEW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/khaledghalwash/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
            "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
            "2025-11-05 12:50:42.935163: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-05 12:50:43.055108: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762339843.107363   44137 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762339843.121587   44137 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762339843.206235   44137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762339843.206268   44137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762339843.206270   44137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762339843.206271   44137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-05 12:50:43.221122: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import joblib\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1yxDUQr3MEY"
      },
      "source": [
        "## 1. Data Collection and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkHvh8A13MEY"
      },
      "outputs": [],
      "source": [
        "def extract_landmarks(image_path):\n",
        "    \"\"\"Extract hand landmarks from an image using MediaPipe\"\"\"\n",
        "    # Initialize MediaPipe Hands\n",
        "    mp_hands = mp.solutions.hands\n",
        "    hands = mp_hands.Hands(\n",
        "        static_image_mode=True,\n",
        "        max_num_hands=1,\n",
        "        min_detection_confidence=0.5\n",
        "    )\n",
        "\n",
        "    # Read image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        return None\n",
        "\n",
        "    # Convert BGR to RGB\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process the image\n",
        "    results = hands.process(image_rgb)\n",
        "\n",
        "    # Extract landmarks\n",
        "    landmarks = []\n",
        "    if results.multi_hand_landmarks:\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "            for landmark in hand_landmarks.landmark:\n",
        "                landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
        "\n",
        "    hands.close()\n",
        "    return landmarks if landmarks else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8clsE5W3MEZ"
      },
      "outputs": [],
      "source": [
        "def process_dataset(data_dir):\n",
        "    \"\"\"Process all images in the dataset directory\"\"\"\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    # Iterate through gesture folders\n",
        "    for gesture_name in os.listdir(data_dir):\n",
        "        gesture_dir = os.path.join(data_dir, gesture_name)\n",
        "        if not os.path.isdir(gesture_dir):\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing {gesture_name}...\")\n",
        "\n",
        "        # Process each image in the gesture folder\n",
        "        for image_name in tqdm(os.listdir(gesture_dir)):\n",
        "            if not image_name.endswith(('.jpg', '.png')):\n",
        "                continue\n",
        "\n",
        "            image_path = os.path.join(gesture_dir, image_name)\n",
        "            landmarks = extract_landmarks(image_path)\n",
        "\n",
        "            if landmarks:\n",
        "                data.append(landmarks)\n",
        "                labels.append(gesture_name)\n",
        "\n",
        "    return np.array(data), np.array(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKH8Ln_w3MEZ"
      },
      "source": [
        "## 2. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyfX--dv3MEZ"
      },
      "outputs": [],
      "source": [
        "def train_model(X, y):\n",
        "    \"\"\"Train and evaluate the Random Forest classifier\"\"\"\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # Initialize and train the model\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model\n",
        "    train_score = model.score(X_train, y_train)\n",
        "    test_score = model.score(X_test, y_test)\n",
        "\n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(model, X, y, cv=5)\n",
        "\n",
        "    # Generate predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    return model, {\n",
        "        'train_score': train_score,\n",
        "        'test_score': test_score,\n",
        "        'cv_scores': cv_scores,\n",
        "        'y_test': y_test,\n",
        "        'y_pred': y_pred\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot-QzlQX3MEa"
      },
      "source": [
        "## 3. Model Evaluation and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEDYydl33MEa"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_test, y_pred, classes):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=classes,\n",
        "                yticklabels=classes)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCFgOe6K3MEa"
      },
      "outputs": [],
      "source": [
        "def plot_feature_importance(model, feature_names):\n",
        "    \"\"\"Plot feature importance\"\"\"\n",
        "    importance = model.feature_importances_\n",
        "    indices = np.argsort(importance)[::-1]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.title('Feature Importance')\n",
        "    plt.bar(range(len(importance)), importance[indices])\n",
        "    plt.xticks(range(len(importance)), [feature_names[i] for i in indices], rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58_EoLTW3MEa"
      },
      "source": [
        "## 4. Main Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GYsjD7O63MEa",
        "outputId": "5bbef1cc-8fb5-49ea-fea6-876799233e6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using dataset from: /home/khaledghalwash/Downloads/depi-project-main/archive(6)\n",
            "Processing train...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 12483.05it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 12483.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing test...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 22550.02it/s]\n",
            "\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m X, y \u001b[38;5;241m=\u001b[39m process_dataset(data_dir)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model, results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[4], line 4\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Train and evaluate the Random Forest classifier\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Split the data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize and train the model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\n\u001b[1;32m     10\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     11\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     12\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m     ):\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    228\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2919\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2916\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2918\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2919\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m   2921\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2499\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2496\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2500\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2501\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2502\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2503\u001b[0m     )\n\u001b[1;32m   2505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ],
      "source": [
        "# Set data directory\n",
        "data_dir = '/home/khaledghalwash/Downloads/depi-project-main/archive(6)/train/train'\n",
        "\n",
        "print(\"Using dataset from:\", data_dir)\n",
        "print(\"\\nProcessing gesture folders...\\n\")\n",
        "\n",
        "# Check if the directory has data\n",
        "if not os.path.exists(data_dir) or not any(os.scandir(data_dir)):\n",
        "    print(\"\\nNo data found in the specified directory!\")\n",
        "else:\n",
        "    # Process dataset\n",
        "    X, y = process_dataset(data_dir)\n",
        "    \n",
        "    if len(X) == 0:\n",
        "        print(\"No valid hand landmarks were extracted from the images.\")\n",
        "    else:\n",
        "        # Train model\n",
        "        model, results = train_model(X, y)\n",
        "\n",
        "        # Print results\n",
        "        print(f\"\\nTraining Score: {results['train_score']:.4f}\")\n",
        "        print(f\"Testing Score: {results['test_score']:.4f}\")\n",
        "        print(f\"Cross-validation Scores: {results['cv_scores']}\")\n",
        "        print(f\"Mean CV Score: {np.mean(results['cv_scores']):.4f}\")\n",
        "\n",
        "        # Print classification report\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(results['y_test'], results['y_pred']))\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        plot_confusion_matrix(results['y_test'], results['y_pred'], np.unique(y))\n",
        "\n",
        "        # Save the model\n",
        "        os.makedirs('models', exist_ok=True)\n",
        "        joblib.dump(model, 'models/hand_gesture_classifier.joblib')\n",
        "\n",
        "        # Save feature names\n",
        "        feature_names = [f'landmark_{i}' for i in range(X.shape[1])]\n",
        "        plot_feature_importance(model, feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Completed (5/6) *Run tests and start app*\n",
        "\n",
        "I'll give clear, copyable steps to run the project locally and with Docker, plus quick troubleshooting tips.\n",
        "\n",
        "## Quick checklist (prerequisites)\n",
        "- Python 3.8+ (project tested with 3.10/3.11; choose one installed locally)\n",
        "- pip\n",
        "- Virtualenv (recommended)\n",
        "- Webcam (for live demo) or adjust CAMERA_INDEX if using a different device\n",
        "- Docker (optional, for container run)\n",
        "\n",
        "## 1) Run locally (recommended for development)\n",
        "1. Open a terminal and change into the project root:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "bash"
        }
      },
      "outputs": [],
      "source": [
        "cd /home/khaledghalwash/Downloads/depi-project-main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "2. Create & activate a virtual environment:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "bash"
        }
      },
      "outputs": [],
      "source": [
        "python -m venv .venv\n",
        "source .venv/bin/activate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "3. Install Python dependencies:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "bash"
        }
      },
      "outputs": [],
      "source": [
        "pip install --upgrade pip\n",
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "4. Prepare environment file (if present):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "bash"
        }
      },
      "outputs": [],
      "source": [
        "cp .env.example .env\n",
        "# Edit .env if you need to change CAMERA_INDEX, FRAME_WIDTH, FRAME_HEIGHT, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "5. Run the test suite (sanity check):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "bash"
        }
      },
      "outputs": [],
      "source": [
        "pytest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see tests pass (in this environment they passed: 4 passed).\n",
        "\n",
        "6. Start the Streamlit app:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "bash"
        }
      },
      "outputs": [],
      "source": [
        "streamlit run app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Streamlit will print a local URL (usually http://localhost:8501). Open that in your browser.\n",
        "- To stop the app, press Ctrl+C in the terminal.\n",
        "\n",
        "If you prefer to run it in the background:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "bash"
        }
      },
      "outputs": [],
      "source": [
        "nohup streamlit run app.py &> streamlit.log &\n",
        "# then tail -f streamlit.log to watch logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 2) Run with Docker (recommended for consistent environment)\n",
        "1. Build the image from the project root:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "bash"
        }
      },
      "outputs": [],
      "source": [
        "docker build -t hand-gesture-app ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "2. Run the container (maps Streamlit port 8501):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "bash"
        }
      },
      "outputs": [],
      "source": [
        "docker run --rm -p 8501:8501 hand-gesture-app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Visit http://localhost:8501 in the browser.\n",
        "- Stop container with Ctrl+C (if running foreground) or docker stop <id>.\n",
        "\n",
        "If you need to expose a local webcam into the container, extra Docker configuration is required (device passthrough or use host network) — tell me if you want that and I’ll show exact docker run flags for your OS.\n",
        "\n",
        "## 3) Environment variables you can change\n",
        "- CAMERA_INDEX — index of the camera (default 0)\n",
        "- FRAME_WIDTH / FRAME_HEIGHT — frame resolution\n",
        "- MAX_NUM_HANDS, MIN_DETECTION_CONFIDENCE, MIN_TRACKING_CONFIDENCE\n",
        "You can set them in `.env` or export them before running, e.g.:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "bash"
        }
      },
      "outputs": [],
      "source": [
        "export CAMERA_INDEX=1\n",
        "streamlit run app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 4) Quick troubleshooting\n",
        "- Model not found or model is None:\n",
        "  - Confirm hand_gesture_classifier.joblib exists. The project loads the model from hand_gesture_classifier.joblib.\n",
        "  - If missing, run the notebook model.ipynb or training scripts to produce a model, or copy a trained `.joblib` into artifacts.\n",
        "- Camera fails to open:\n",
        "  - Try different `CAMERA_INDEX` values (0, 1, ...).\n",
        "  - Verify webcam permissions and that no other process is using it.\n",
        "  - On headless servers (no camera), you can run but no live video will appear.\n",
        "- Mediapipe / opencv install issues:\n",
        "  - On Linux, installing system libs helps (Dockerfile includes apt packages used to avoid issues): libgl1, libsm6, libxext6, libglib2.0-0, ffmpeg.\n",
        "- Port already in use (8501):\n",
        "  - Change Streamlit port with environment var:\n",
        "    ```bash\n",
        "    export STREAMLIT_SERVER_PORT=8502\n",
        "    streamlit run app.py\n",
        "    ```\n",
        "- If Streamlit logs \"Model not loaded\" but you expect it to be, check for errors printed at startup (permission, corrupted joblib, version mismatch of scikit-learn). Re-train model with the same scikit-learn version if necessary.\n",
        "\n",
        "## 5) Useful verification commands\n",
        "Show model file exists:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "bash"
        }
      },
      "outputs": [],
      "source": [
        "ls -l artifacts/hand_gesture_classifier.joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "Run tests (already done earlier):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "bash"
        }
      },
      "outputs": [],
      "source": [
        "pytest -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 6) Next steps I can do for you\n",
        "- Start the app in this environment now and stream logs here (confirm and I’ll launch it).\n",
        "- Help generate a lightweight systemd / process file to run it as a service.\n",
        "- Add a small health-check endpoint or improve lazy model loading so the app starts even without the model present.\n",
        "\n",
        "Which next action do you want?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
